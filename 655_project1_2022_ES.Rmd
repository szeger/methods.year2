---
output:
  pdf_document: default
  html_document: default
---

####Exploring Longitudinal Data from the    
####Childhood Asthma Management Program (CAMP)    
####Biostat655 Project1 2022
######Elizabeth Colantuoni, Jisoo Kim, Emily Scott, Scott Zeger
######6/9/2021




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(nlme)
```

###### Introduction
The Childhood Asthma Management Program (CAMP) was a multicenter, randomized controlled trial designed to determine the long-term effects of three treatments: budesonide, nedocromil or placebo on pulmonary function among children with asthma. Children with asthma aged
5-12 years were enrolled between 1993 and 1995. The primary outcome  was *forced expiratory volume at one second (FEV1)* measured soon after the oral administration of the treatment using a bronchodilator . Children were randomized to receive one of the three treatments and then followed for 4 years in the primary study. A subset of children were followed up for an additional 5 years in
the continuation study. The primary outcome was measured at baseline and then at 2, 4, 12, 16, 24, 28, 36, 40 and 48 months after randomization during the primary study. During the continuation study, additional FEV1 measures were made at 52, 60, 72, 84, 96 and 108 months from randomization.

References:

1. The Childhood Asthma Management Program (CAMP): Design, Rationale, and Methods. Controlled
Clinical Trials 1999; 20:91-120.
2. Long-Term Effects of Budesonide or Nedocromil in Children with Asthma. New England Journal of
Medicine 2000;343:1054-63.

###### Data Description
The dataset comprises a random sample of 695 children from CAMP. The dataset includes the primary outcome (post-bronchodilator FEV1), treatment group, as well as baseline characteristics of the children.
The comma delimited files *camp_primary.csv* and *camp_continuation.csv* hold the data from months 0 through 48 and 50 through 108, respectively.

The following variables are available for each child visit:

* *id*: participant ID
* *trt*: treatment group: 0-placebo; 1-budesonide; 2-nedocromil
* *age_rz*: age in years at randomization 
* *gender*: 0-male; 1-female
* *ethnicity*: 0-White; 1-Black; 2-Hispanic; 3-Other
* *posfev*: primary outcome: post-bronchodilator forced experiratory volume in one second (FEV1) in liters
* *visit*: visit number 
* *visitc*: months since randomization of visit
* *fdays*: days since randomization of visit
```{r read the primary study dataset into R}
setwd("/Users/szeger/OneDrive - Johns Hopkins University/professional/courses/bio650s/bio655.2022/projects/project1")
#
#
df = read.csv(file = "camp_primary.csv")
df = df %>%
  rename(female=gender, fev=POSFEV)
head(df)
dim(df); dim(df[complete.cases(df),])
df.cc=df[complete.cases(df),]
```
An first step in a data analysis project is to become more familiar with the database. Here, we can see there are  want to check that each variable has the expected type: numeric or character; continuous or discrete; learn about the shape of the distribution of the key continuous variables; discover unusual values that might reflect real rare events or possibly coding errors. We look at the frequencies of the discrete variables and examine the relationships among key variables, for example by visualizing joint frequencies or distributions of pairs of variables. 

Here are a few simple tabular and visual displays to explore the dataset. 
```{r make exploratory displays for checking data validity}
# tabulate the number of observations over time for each treatment group
tab1=table(df$trt,df$visit)
tab1.m1=table(df$visit)
tab1.m2=table(df$trt)
tab1.all=rbind(tab1,tab1.m1)
rownames(tab1.all)[4]="total"
tab1.all=cbind(tab1.all,c(tab1.m2,sum(tab1.m2)))
colnames(tab1.all)[11]="total"
print(tab1.all)
#
temp=t(tab1.all);temp=temp[-nrow(temp),-ncol(temp)]
matplot(temp,type="b",pch=c("0","1","2"),ylab="Numbers",xlab="Visit",ylim=c(0,300),las=1)
#
```

###### Question, Question, Question
In real estate, the 3 most important considerations are: location, location, location. In statistical analysis, the 3 most important are: question, question, question.

The question for this analysis of the  of the primary CAMP study is: *are greater improvements in pulmonary function
over time with the use of budesonide or nedocromil compared to placebo in children with asthma*. 

In randomized trials, especially those used for licensing interventions, the statistical analysis plan would be pre-specified, such that an analyst would conduct a pre-specified exploratory and confirmatory analysis of the data. Pre-specified analysis plans are good practice both in experiments and observational studies. However, we will be analyzing the data of this trial with the primary objective above but without a pre-specified analysis plan to allow you to gain experience exploring the mean, variance and
correlation in longitudinal data, as well as specifying, implementing and interpreting regression models for longitudinal data.

###### Visualizing longitudinal data
As with ordinary regression, in Longitudinal data analysis (LDA), we also seek to describe the conditional distribution $[Y|X]$ of outcome variables $(Y)$ given predictor variables $(X)$. Because the outcome $Y=(Y_1,...Y_{n_i})$ is an $n_i x 1$ vector of repeated measurements for person $i$, there are many conditional means to study. These include: 
* $[Y_{ij}|X_{ij}]$, the conditional distribution of the outcome at time $t_{ij}$ for person $i$ given the predictor variables at the same time. *Marginal* models focus on the conditional mean of this distribution.

* $[Y_{ij}|Y_{i1},\ldots,Y_{ij-1},X_{ij}]$, the conditional distribution of outcome $j$ for person $i$ given the predictor variables at the same time and the history of prior responses. *Transition* models focus on this mean.

* $[Y_{ij}|b_i,X_{ij}]$, the conditional distribution of outcome $j$ for person $i$ given the predictor variables at the same time and unobserved random effects $b_i$ that represent latent variables that explain the heteogeneity in the patterns of responses among people. *Random effects or mixed* models focus on this conditional mean.

An starting point is to visualize the conditional mean response of interest and covariance given time-independent (e.g. baseline) predictors:

* $E(Y_i|X_{i0})$

* $\mbox{Cov}(Y_i|X_{i0})$.

First, let's make a picture of the observed vector of data over time. The *spaghetti plot* plots $Y_{ij}$ versus $t_{ij}$ for all of the observations $j=1,\ldots,n_i$ for each person $i=1,\ldots,m$, connecting tne repeated values on the same person with a line. Note it is important to use thin, transparent lines given there are 659 persons each with 10 or more observations.

```{r spaghetti plot, echo=FALSE}
ggplot(df,aes(x=visitc,y=fev,group=id,col=as.factor(female))) + 
  geom_line(alpha=0.1) + xlab("Visit") + ylab("FEV1") + labs(title ="Spaghetti Plot of FEV1 by Sex")
```

###### Examining the data for evidence of treatment differences
To begin to investigate the treatment effects, we can make the spaghetti plot identifying the treatment group by color and adding the estimates of the mean curvse for each of the three treatment groups.
```{r spaghetti plot with treatment means, echo=FALSE}
ggplot(df,aes(x=visitc,y=fev,group=id,col=as.factor(trt))) + 
  geom_line(alpha=0.05) + xlab("Visit") + ylab("FEV1") + labs(title ="Spaghetti Plot of FEV1 by Treatment Group") +
  geom_smooth(aes(x=visitc,y=fev,group=as.factor(trt)))
```

###### Boxplots of treatment differences over time
It may be easier to understand the conditional distribution of *fev* given follow-up time and treatment group by comparing boxplots. Here is an example.
```{r boxplots by time and treatment}
# distribution of the outcome fev versus visit for the three treatment groups
df_summaries <- df.cc %>% 
  mutate(visitc_new = ifelse(trt == 0, visitc - 0.5,ifelse(trt == 1, visitc, visitc + 0.5))) %>%
  group_by(visitc_new) %>%
  summarize(mean_fev = mean(fev),
            sd_fev = sd(fev),
            n = n())
df.cc %>%
  mutate(visitc_new = ifelse(trt == 0, visitc - 0.5,
                        ifelse(trt == 1, visitc, visitc + 0.5))) %>%
  ggplot(aes(x=visitc_new, y=fev, group = visitc_new)) + 
    geom_boxplot(aes(fill = as.factor(trt))) +
    xlab("Month of visit") + ylab("FEV1") + 
    ggtitle("Boxplots of FEV1 by Treatment and Visit") + theme_bw() + 
    geom_pointrange(data=df_summaries,size=0.1,color="black",
        aes(x = visitc_new, 
            y = mean_fev, 
            ymin = mean_fev - 2*sd_fev/sqrt(n), 
            ymax = mean_fev + 2*sd_fev/sqrt(n)))
```

###### Longitudinal data models
In this section, we will fit a series of regression models to continue our exploration of the $E(Y_{ij}|X_{ij})$. To start, let's ignore the autocorrelation among repeated observations for each individual and use ordinary least squares (OLS). As there is not any missing data, the OLS estimates of the regression coefficients are unbiased. But the inferences about the true coefficients are not correct because they are based upon the false assumption that observations, including those repeated on the same person, are independent of one another. 

The model must adequately represent the scientific question. In this case, we want to learn from the study how the mean *fev* trajectory is different among the three treatment groups. We do not want to assume that the trajectories are linear so should use a model that allows for a different curvilinear relationship with follow-up time in each group. 

A relatively simple model that fits these requirements is to let each groups mean curve be a quadratic function of follow-up time. This is accomplished by including a 2 df main effect for time represented by $(t, t^2)$ and the interaction of these time variables with indicators for the 3 groups. 

Because the study is a randomized trial, we know that the three groups have the same expected value at time 0. The implication of this prior knowledge is that we know the main effect for treatment is 0. So we should not estimate it in our model. This case is an example were we want the interaction between time and group, but not the main effect for group in the model. So our proposed model for the conditional mean of $Y$ given $X$ is
\[
E(Y_{ij}|X_{ij}) = \beta_0 + (t \beta_{1.1}+ t^2 \beta_{1.2}) + 1_{\{group=1\}}(t \beta_{2.1}+ t^2 \beta_{2.2}) + 1_{\{group=2\}}(t \beta_{3.1}+ t^2 \beta_{3.2})
\]
Here, $t \beta_{1.1}+ t^2 \beta_{1.2}$ is a smooth (quadratic) function of time for the control group mean *fev* and $t \beta_{2.1}+ t^2 \beta_{2.2}, t \beta_{3.1}+ t^2 \beta_{3.2}$ are the differences between groups 1 and 2 versus control for the two treatment groups.
```{r fit least square}
df.cc = df.cc %>% mutate(time=(visitc/48),time2=time*time)
reg.ols1a = lm(data=df.cc,fev ~ time + time2 + (time + time2):as.factor(trt))
sum.ols1a=summary(reg.ols1a)
sum.ols1a
reg.ols1b = lm(data=df.cc,fev ~ time  + time:as.factor(trt))
sum.ols1b=summary(reg.ols1b)
reg.ols2 = lm(data=df.cc,fev ~ time + time:as.factor(trt))
sum.ols2=summary(reg.ols2)
sum.ols2
#
```

###### OLS Results
There is little evidence of curvature in any of the groups, so we refit the model without $t^2$ and will now focus on this simplified model. The control group has a positive slope increasing 0.98 liters over the 48 month period. This represents an increase of $54\% = 0.98/1.81$ relative to the common baseline level (as estimated for all three groups) by the model intercept. The difference in lung growth between Group 1 and control is negligble as evidenced by the slope of $0.011 (\mbox{reported se}=0.0348)$. The growth rate difference for Group 2 relative to control is estimated to be $-0.091 (\mbox{reported se}=0.0351)$ that is $9\%$ = -0.091/0.997$ lower than the estimated control group growth rate.

Because the repeated observations for a child are not independent and because *OLS* assumes they are, inferences from *OLS* are invalid. So we can not trust the OLS standard errors, p-values, confidence intervals or tests of hypothes. Said another way, the *OLS* assumes there is not heterogeity in the true growth rates among children and that any apparent differences are due entirely to random measurement error. This critical assumption is false.

A second, possibly less critical incorrect assumption is that the variance of the errors is constant for all observations. But we can see in the boxplots and spaghetti plots that the variance increases as children age, that is as they get bigger. This makes sense because lung function grows at different rates among children so the heterogeneity in *fev* at birth is likely to increase with time.

We can retain the same model for the mean because it addresses our question, but make a more valid assumption about the *autocorrelation* among repeated observations for a child. Once, we choose that assumption, we can use *generalized least squres* or *GLS* to estimate the growth rates for the three treatment groups while accounting for the autocorrelation and unequal variances. If the repeated measures on a child were actually independent but with variance that changed over time, we could use *weighted least squares* or *WLS* instead of *OLS* or *GLS*. (So *WLS* is a special case of *GLS* and *OLS* is a special case of *WLS*; that is *GLS* $\supseteq$ *WLS* $\supseteq$ *OLS*.)

###### Generalized Least Squares

To account for the correlation and unequal variances, we need to simultaneously model the mean and covariance of the vector of outcomes for each individual as a function of the covariates $X$. Thus far, we are relatively satisfied with our model for the mean. On the other hand, the *OLS* model for the $n_i \times n_i$ matrix $\mbox{var}(Y_i|X_i) = \Sigma_i$ can be written $\Sigma_i = \sigma^2 I_{n_i})$. To see this fact, recall the $(j,k)$-element of $\Sigma_i$ is the covariance of $Y_{ij}$ with $Y_{ik}$. The diagonal entries in $\Sigma_i$, where $j=k$, are the variances of the observations. The off-diagonal elements are the covariances between each pair of responses. Because the *OLS* model assumes constant variances $\sigma^2$ and pair-wise independence, it corresponds to assuming $\Sigma_i = \sigma^2 I_{n_i}$.

We will discuss several parsimonious models for covariance matrices later in the course. In this problem where $n_i = n$ and the observation times are the same for all individuals,  we can assume $\Sigma_i = \Sigma$ for all $i$ and estimate the $n$ variances and $n(n-1)/2$ covariances without making any further simplifying assumptions. We can estimate the covariance matrix using
\[
\hat{\sigma}_{jk} = \hat{cov}(Y_{ij},Y_{ik}) = \Sigma_{i=1}^m (y_{ij}-x_{ij}\hat{\beta})(y_{ik}-x_{ik}\hat{\beta})
\]
where $\hat{\beta}$ are the estimated regression coefficients, say from *OLS*.

Given an estimator $\hat{\Sigma}$, how do we then get the *WLS* estimate of $\beta$? Recall that *OLS* maximizes the likelihood of the responses as a function of the unknown $\beta$s. Assuming the responses follow a Gaussian distribution, maximizing the likelihood of the data corresponds to minimizing the sum of squares of the residuals.

The generalized regression model can be written
$Y_i = X_i \beta + \epsilon_i$ with $\mbox{var}(Y_i|X_i) = \Sigma.$ Let $L$ be an $n \times n$ matrix satisfying $\Sigma^{-1} = LL$. Since multiplying $L$ by itself gives $\Sigma^{-1}$, we say $L=\Sigma^{-1/2}$.  If we multiple the regression model above by $L$, we get $LY_i = LX_i \beta + L\epsilon_i$ where $var(L\epsilon_i) = L\Sigma L'= I$. So we now have a regression of $LY_i$ on $LX_i$ with the original regression coefficients $\beta$ and errors $L\epsilon_i$ that are independent and have common variance 1. Multiply the $Y$s and $X$s by $L$ decorrelates them and standardizes their variances. So, we can use *OLS* to regress $LY_i$ on $LX_i$. Doing so is called *generalized least squares*. In practice, one needs to get an initial estimate of $\beta$ to get a estimate of $\Sigma$, then we iteratively use the $\hat{\Sigma}$ to get a better estimate of $\beta$ and so on until the estimates do not change. 

The *GLS* results give unbiased estimates of the true regression coefficients *and of their uncertainties*. Let's see how this turns out for the CAMP data.

```{r fit model using generalized least squares}
#
#
reg.gls1 = gls(data=df.cc,
               fev ~ time + time:as.factor(trt),
               correlation=corSymm(form=~visit|id)
               )
sum.gls1 = summary(reg.gls1)
sum.gls1
reg.gls2 = gls(data=df.cc,
               fev ~ time*as.factor(trt),
               correlation=corSymm(form=~visit | id)
               )
sum.gls2 = summary(reg.gls2)
sum.gls2
#
#
# make a table to compare ols to gls for model 2
#
result1=cbind(sum.ols1b$coefficients[,1:2],sum.gls1$coefficients,sqrt(diag(sum.gls1$varBeta)))
colnames(result1)=c("est.ols","se.ols","est.gls","se.gls")
```
###### 
After the strong warnings about *OLS* producing invalid inferences from autocorrelated data, the estimates and standard errors from and *GLS* are very similar to those from *OLS* except for the intercept. So what gives? We need to refine our understanding slightly. *GLS* will give correct inferences when its assumed covariance model is close to the truth; *OLS* will given invalid inferences except in study designs where the autocorrelation is irrelevant. In this design, every person has identical times of measurement and there is no missing data.  

```{r fit model for age at randomization}
df.cc = df.cc %>% mutate(age_visit=age_rz+visitc/12)
df.cc=df.cc %>% ungroup() %>% mutate(age_visit_c=age_visit-mean(age_visit),age_visit2=age_visit_c^2)
ggplot(data=df.cc,aes(x=age_visit_c, y =fev, group=id)) + geom_line(alpha=0.1) + xlab("Age (years)") + ylab("FEV") 
#+ geom_abline(intercept=2.22,slope=.0736,col="red") + #geom_abline(intercept=2.22,slope=.22,col="green")
#
reg.ols3 = lm(data=df.cc,
               fev ~ age_visit_c
               )
sum.ols3 = summary(reg.ols3)
sum.ols3
reg.gls3 = gls(data=df.cc,
               fev ~ age_visit_c,
               correlation=corSymm(form=~visit | id)
               )
sum.gls3 = summary(reg.gls3)
sum.gls3
#
result2=cbind(sum.ols3$coefficients[,1:2],sum.gls3$coefficients,sqrt(diag(sum.gls3$varBeta)))
